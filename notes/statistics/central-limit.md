# 中心極限定理について話そう

- [中心極限定理について話そう](#中心極限定理について話そう)
  - [登場人物](#登場人物)
  - [中心極限定理とは](#中心極限定理とは)
    - [母平均$\mu$を知りたい](#母平均muを知りたい)
    - [$E(\bar{X})=\mu$なんです](#ebarxmuなんです)
    - [なぜ$n$の大きさが重要なのか](#なぜnの大きさが重要なのか)
    - [$\bar{X}$の確率変数は正規分布に従う](#barxの確率変数は正規分布に従う)
    - [なぜ$\bar{X}$の確率変数が正規分布に従うとうれしいのか](#なぜbarxの確率変数が正規分布に従うとうれしいのか)
  - [まとめ：中心極限定理とは](#まとめ中心極限定理とは)
  - [補足：混同しやすい点](#補足混同しやすい点)

## 登場人物

おばあさん：中心極限定理に詳しい。  
お嬢さん：おばあさんに質問をする。

## 中心極限定理とは

### 母平均$\mu$を知りたい

お嬢さん：中心極限定理ってなんですか。

おばあさん：中心極限定理の前提は、母集団に確率変数が存在するということです。

お嬢さん：確率的に変動するなんらかの値があり、その平均や分散などを知りたいということですね。

おばあさん：そのとおりです。ここでは知りたい統計量を母平均だけに絞り、それを$\mu$と表記しましょう。しかし直接母集団の確率変数のデータをあつめて、計算することは現実には困難な場合が多いです。

お嬢さん：データをあつめて...? それで私もデータをあつめて...? 母集団が大きすぎたりすると難しいでしょうね。

おばあさん：そうです。それでどうするかというと、母集団をランダム・サンプリングして、標本（サンプル）をとるんです。ここではサンプルサイズ$n$としておきましょう。そして標本平均

$\bar{X} = \frac{X_1 + X_2 + \cdots + X_n}{n}$

を計算することで、$\mu$を推測しようとするんです。

### $E(\bar{X})=\mu$なんです

お嬢さん：まあ感覚的にはわかりますね。ランダム・サンプリングされていれば、標本の構成と母集団の構成の共通性は高くなるでしょうから。しかしランダム性が確保されているとはいえ、なぜ限られた標本から母集団の確率分布を推測できるんでしょうか？

おばあさん：感覚的にわかる、その感じが重要なんです。それこそ感覚的にいえば、$\bar{X}$と$\mu$は一致しそうな気がしませんか...?

お嬢さん：そうかもしれません。しかし、現実には標本は1回しかとれないことが多いでしょう。そこでたまたま、偏りの大きい標本を引いてしまい、$\bar{X}$が$\mu$と比べて大きすぎたり小さすぎたりすることもあるのでは...?

おばあさん：まったくそのとおりです。なのでここでは、サンプリングを何回も行える、という仮想的な前提を導入しましょう。

お嬢さん：何回もサンプリングを行い、$\bar{X}$を毎回毎回算出して並べてみれば、平均的には$\bar{X}$の値は$\mu$に近づきそうですね。  

おばあさん：改めて言い直してみると、まず$\bar{X}$の大きさ自体が、標本を引くときの偏りによって確率的に変動するわけです。しかしその偏りというのは、$\mu$を基準として大きい場合と小さい場合があり、何回もサンプリングするとその両ケースが打ち消し合うために、平均的には$\mu$に近づいていくんです。

お嬢さん：フォーマライズすると、平均$\mu$、分散$\sigma^2$の母集団から得られた標本の仮想的確率変数$\bar{X}$の期待値$E(\bar{X})$が$\mu$と一致するということですね、今までの話は。

おばあさん：へーそうなんですか。

お嬢さん：急にどうしたんだ。しっかりしてくれよ。だいたい「躁なんですか」ってなんですか。北杜夫に訊く質問かよ。

おばあさん：じゃあ次行きませう。

### なぜ$n$の大きさが重要なのか

おばあさん：今までの話だと、とりあえず、標本を何回もとって、$E(\bar{X})$を計算すれば$\mu$は確実に推定できそうだということでしたね。

お嬢さん：しかし、繰り返しになりますがこれは非現実的な話ですよね。基本的には1回しか標本はとれないのだから。となると$\mu$から離れた値が$\bar{X}$として出現する可能性も、すごく低いというわけではなさそうです。

おばあさん：安心してください。

お嬢さん：といいますと。

おばあさん：その心配は、サンプルサイズ$n$を大きくすることで解決できます。

お嬢さん：たったそれだけで。

おばあさん：といいますのは、サンプルサイズ$n$で繰り返しサンプリングして、$\bar{X}$の確率変数をつくると、その分散$V(\bar{X})$は$\frac{\sigma^2}{n}$に一致するからなんです。

お嬢さん：ちょっとまだ含意をつかみかねているんですが。

おばあさん：要は、サンプルサイズが大きくなるほど$\bar{X}$の確率変数の分散は小さくなる、言い換えれば$\mu$近傍の値をとる可能性が高くなるということです。

お嬢さん：つまり、一回きりのサンプリングから得られた$\bar{X}$でも、$n$が大きければ$\mu$に近い可能性が高くなるので、$n$を増やせば推定の精度を上げられると...?

おばあさん：そういうことなんです。

お嬢さん：サンプルサイズの大きさが重要なんですね。

### $\bar{X}$の確率変数は正規分布に従う

お嬢さん：しかし、ここまでだったら、すでに学習した、独立同一分布からの確率変数の相加平均の確率変数の性質を述べてるだけですよね。平均$\mu$、分散$\sigma^2$の母集団からの確率変数$X_1, \cdots, X_n$の相加平均の確率変数$\bar{X}$の平均$E(\bar{X})$が$\mu$、分散$V(\bar{X})$が$\frac{\sigma^2}{n}$になるっていう。

おばあさん：するどい指摘です。つまり、中心極限定理を中心極限定理たらしめている性質はまだ説明されていないのではないかと。

お嬢さん：そのとおりです。

おばあさん：じゃあお見せしましょう。中心極限定理は、$\bar{X}$の確率変数が平均$\mu$、分散$\frac{\sigma^2}{n}$の分布をする、というだけではなく、じつは$\bar{X}$の確率変数は正規分布$N(\mu, \frac{\sigma^2}{n})$にしたがう、ということをいっている定理なのです。

お嬢さん：ほう、正規分布ですか...。

おばあさん：なんで正規分布になるのか、その証明は、今回の私の任務を超えていますので割愛させていただきます。

お嬢さん：そもそもわかってるんですか？

おばあさん：いや、ほぼわかってません。次回までに勉強してきます。で、$\bar{X}$の確率変数が正規分布することには大きなメリットがあるんです。

お嬢さん：ほう。

### なぜ$\bar{X}$の確率変数が正規分布に従うとうれしいのか

おばあさん：$\bar{X}$の確率変数が正規分布するメリットを順を追って説明したいと思います。お嬢さん、いままでの説明で、本当に中心極限定理が使えるものなのかどうか、まだ腑に落ちてないんじゃないですか？

お嬢さん：こういう趣旨のことをいうのは3度目ですが、いくらサンプルサイズ$n$が大きくても、あくまで現実ではサンプリングは1回きりなのだから、$\bar{X}$が$\mu$と必ず一致するとは言い切れないという点がやはり引っかかりますね。

おばあさん：それは全くそのとおりで、実際には$\mu$は$\bar{X}$より少し小さかったり、大きかったりすることがほとんどでしょう。測定をどこで切り上げるかにもよりますが。したがって、実際には「$\mu$は$\bar{X}$の周辺に位置する可能性が高い」というような言い方しかできません。

お嬢さん：「周辺」に位置する可能性が「高い」って随分曖昧な言い方ですね。「周辺」ってどのあたりなんだ。そして「高い」ってのも定量化してほしいですね。こんなんじゃやっぱり、中心極限定理もあんまり使えないんじゃないか？

おばあさん：安心してください。$\bar{X}$の確率変数は正規分布$N(\mu, \frac{\sigma^2}{n})$に従うわけですが、$\bar{X}$を線形変換することで標準正規分布$N(0, 1)$に従う確率変数（標準化変数）に変換できます。標準正規分布の累積分布関数は広く知られているので、これを用いて$\bar{X}$の「周辺」のどの程度の範囲がどの程度の確率、つまり$\mu$が含まれうる「可能性の高さ」を持っているのかを定量化できます。

お嬢さん：いいですね。標準化っていう作業ですね。一般に確率変数$X$の標準化変数$Z$は

$Z = \frac{X - E(X)}{\sqrt{V(X)}}$

で求まり、今回は確率変数$\bar{X}$について$E(\bar{X})=\mu, V(\bar{X})=\frac{\sigma^2}{n}$なので、

$Z = \frac{\bar{X} - \mu}{\sigma / \sqrt{n}}$

で変換できますね。

おばあさん：さよう。このつづきは再度。

お嬢さん：では、内容をまとめておきます。

## まとめ：中心極限定理とは

1. **中心極限定理**とは、平均$\mu$、分散$\sigma^2$の確率変数をもつ母集団からサンプルサイズ$n$の標本をとり、標本平均$\bar{X} = \frac{X_1 + \cdots + X_n}{n}$を記録する作業を（たとえば100回）繰り返し、（その100個の）$\bar{X}$をひとつの確率変数とみなすと、$\bar{X}$の確率変数の確率分布が$N(\mu, \frac{\sigma^2}{n})$に従うという定理である。

2. 実際には標本をとる作業は1回しかできないが、標本を1回だけとっても、$n$が十分に大きければ$\bar{X}$は母平均$\mu$の付近の値である可能性が高い。

3. $n$が小さめでも平均的には$\bar{X}$は$\mu$に一致するが、分散$V(\bar{X}) = \frac{\sigma^2}{n}$が大きめになるため$\mu$の推定は不正確になる。だからある程度$n$が大きいほうが、$\mu$の推定の精度は上がる。

## 補足：混同しやすい点

1. 中心極限定理に関係するのは標本平均$\bar{X} = \mu$のみであり、標本分散

    $\frac{1}{n-1}\{(X_1 - \bar{X})^2 + (X_2 - \bar{X})^2 + \cdots + (X_n - \bar{X})^2\} = s^2$

    は関係しない。中心極限定理でいう「標本平均の分布の分散」は$V(\bar{X})$のことを指しており「標本分散」$s^2$のことではない。

    それにしても混同しやすいが、この混同しやすさは、「標本平均の分布」と「標本分布」の混同しやすさから来ていると思う。前者は実質的に仮想的な概念だが、後者は観測可能である、という違いを抑えていれば理解しやすくなるのではないか。

2. 中心極限定理で重要なのはサンプルサイズ$n$の大きさであって、$\bar{X}$の確率変数を考えるときに仮想的に導入した標本数（上のまとめでは100とした）ではない。

    標本数が増えれば、$\bar{X}$が$N(\mu, \frac{\sigma^2}{n})$に従う様子が$\bar{X}$のヒストグラム上でよりわかりやすくなるだろうが、それだけである。標本数が増えれば$V(\bar{X})$が小さくなる、というような関係はない。
