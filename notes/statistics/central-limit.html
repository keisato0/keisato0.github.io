<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>central-limit</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="中心極限定理について話そう">中心極限定理について話そう</h1>
<ul>
<li><a href="#中心極限定理について話そう">中心極限定理について話そう</a>
<ul>
<li><a href="#登場人物">登場人物</a></li>
<li><a href="#中心極限定理とは">中心極限定理とは</a>
<ul>
<li><a href="#母平均muを知りたい">母平均<span class="math inline"><em>μ</em></span>を知りたい</a></li>
<li><a href="#ebarxmuなんです"><span class="math inline"><em>E</em>(<em>X̄</em>) = <em>μ</em></span>なんです</a></li>
<li><a href="#なぜnの大きさが重要なのか">なぜ<span class="math inline"><em>n</em></span>の大きさが重要なのか</a></li>
<li><a href="#barxの確率変数は正規分布に従う"><span class="math inline"><em>X̄</em></span>の確率変数は正規分布に従う</a></li>
<li><a href="#なぜbarxの確率変数が正規分布に従うとうれしいのか">なぜ<span class="math inline"><em>X̄</em></span>の確率変数が正規分布に従うとうれしいのか</a></li>
</ul></li>
<li><a href="#まとめ中心極限定理とは">まとめ：中心極限定理とは</a></li>
<li><a href="#補足混同しやすい点">補足：混同しやすい点</a></li>
</ul></li>
</ul>
<h2 id="登場人物">登場人物</h2>
<p>おばあさん：中心極限定理に詳しい。<br />
お嬢さん：おばあさんに質問をする。</p>
<h2 id="中心極限定理とは">中心極限定理とは</h2>
<h3 id="母平均muを知りたい">母平均<span class="math inline"><em>μ</em></span>を知りたい</h3>
<p>お嬢さん：中心極限定理ってなんですか。</p>
<p>おばあさん：中心極限定理の前提は、母集団に確率変数が存在するということです。</p>
<p>お嬢さん：確率的に変動するなんらかの値があり、その平均や分散などを知りたいということですね。</p>
<p>おばあさん：そのとおりです。ここでは知りたい統計量を母平均だけに絞り、それを<span class="math inline"><em>μ</em></span>と表記しましょう。しかし直接母集団の確率変数のデータをあつめて、計算することは現実には困難な場合が多いです。</p>
<p>お嬢さん：データをあつめて…? それで私もデータをあつめて…? 母集団が大きすぎたりすると難しいでしょうね。</p>
<p>おばあさん：そうです。それでどうするかというと、母集団をランダム・サンプリングして、標本（サンプル）をとるんです。ここではサンプルサイズ<span class="math inline"><em>n</em></span>としておきましょう。そして標本平均</p>
<p><span class="math inline">$\bar{X} = \frac{X_1 + X_2 + ... + X_n}{n}$</span></p>
<p>を計算することで、<span class="math inline"><em>μ</em></span>を推測しようとするんです。</p>
<h3 id="ebarxmuなんです"><span class="math inline"><em>E</em>(<em>X̄</em>) = <em>μ</em></span>なんです</h3>
<p>お嬢さん：まあ感覚的にはわかりますね。ランダム・サンプリングされていれば、標本の構成と母集団の構成の共通性は高くなるでしょうから。しかしランダム性が確保されているとはいえ、なぜ限られた標本から母集団の確率分布を推測できるんでしょうか？</p>
<p>おばあさん：感覚的にわかる、その感じが重要なんです。それこそ感覚的にいえば、<span class="math inline"><em>X̄</em></span>と<span class="math inline"><em>μ</em></span>は一致しそうな気がしませんか…?</p>
<p>お嬢さん：そうかもしれません。しかし、現実には標本は1回しかとれないことが多いでしょう。そこでたまたま、偏りの大きい標本を引いてしまい、<span class="math inline"><em>X̄</em></span>が<span class="math inline"><em>μ</em></span>と比べて大きすぎたり小さすぎたりすることもあるのでは…?</p>
<p>おばあさん：まったくそのとおりです。なのでここでは、サンプリングを何回も行える、という仮想的な前提を導入しましょう。</p>
<p>お嬢さん：何回もサンプリングを行い、<span class="math inline"><em>X̄</em></span>を毎回毎回算出して並べてみれば、平均的には<span class="math inline"><em>X̄</em></span>の値は<span class="math inline"><em>μ</em></span>に近づきそうですね。</p>
<p>おばあさん：改めて言い直してみると、まず<span class="math inline"><em>X̄</em></span>の大きさ自体が、標本を引くときの偏りによって確率的に変動するわけです。しかしその偏りというのは、<span class="math inline"><em>μ</em></span>を基準として大きい場合と小さい場合があり、何回もサンプリングするとその両ケースが打ち消し合うために、平均的には<span class="math inline"><em>μ</em></span>に近づいていくんです。</p>
<p>お嬢さん：フォーマライズすると、平均<span class="math inline"><em>μ</em></span>、分散<span class="math inline"><em>σ</em><sup>2</sup></span>の母集団から得られた標本の仮想的確率変数<span class="math inline"><em>X̄</em></span>の期待値<span class="math inline"><em>E</em>(<em>X̄</em>)</span>が<span class="math inline"><em>μ</em></span>と一致するということですね、今までの話は。</p>
<p>おばあさん：へーそうなんですか。</p>
<p>お嬢さん：急にどうしたんだ。しっかりしてくれよ。だいたい「躁なんですか」ってなんですか。北杜夫に訊く質問かよ。</p>
<p>おばあさん：じゃあ次行きませう。</p>
<h3 id="なぜnの大きさが重要なのか">なぜ<span class="math inline"><em>n</em></span>の大きさが重要なのか</h3>
<p>おばあさん：今までの話だと、とりあえず、標本を何回もとって、<span class="math inline"><em>E</em>(<em>X̄</em>)</span>を計算すれば<span class="math inline"><em>μ</em></span>は確実に推定できそうだということでしたね。</p>
<p>お嬢さん：しかし、繰り返しになりますがこれは非現実的な話ですよね。基本的には1回しか標本はとれないのだから。となると<span class="math inline"><em>μ</em></span>から離れた値が<span class="math inline"><em>X̄</em></span>として出現する可能性も、すごく低いというわけではなさそうです。</p>
<p>おばあさん：安心してください。</p>
<p>お嬢さん：といいますと。</p>
<p>おばあさん：その心配は、サンプルサイズ<span class="math inline"><em>n</em></span>を大きくすることで解決できます。</p>
<p>お嬢さん：たったそれだけで。</p>
<p>おばあさん：といいますのは、サンプルサイズ<span class="math inline"><em>n</em></span>で繰り返しサンプリングして、<span class="math inline"><em>X̄</em></span>の確率変数をつくると、その分散<span class="math inline"><em>V</em>(<em>X̄</em>)</span>は<span class="math inline">$\frac{\sigma^2}{n}$</span>に一致するからなんです。</p>
<p>お嬢さん：ちょっとまだ含意をつかみかねているんですが。</p>
<p>おばあさん：要は、サンプルサイズが大きくなるほど<span class="math inline"><em>X̄</em></span>の確率変数の分散は小さくなる、言い換えれば<span class="math inline"><em>μ</em></span>近傍の値をとる可能性が高くなるということです。</p>
<p>お嬢さん：つまり、一回きりのサンプリングから得られた<span class="math inline"><em>X̄</em></span>でも、<span class="math inline"><em>n</em></span>が大きければ<span class="math inline"><em>μ</em></span>に近い可能性が高くなるので、<span class="math inline"><em>n</em></span>を増やせば推定の精度を上げられると…?</p>
<p>おばあさん：そういうことなんです。</p>
<p>お嬢さん：サンプルサイズの大きさが重要なんですね。</p>
<h3 id="barxの確率変数は正規分布に従う"><span class="math inline"><em>X̄</em></span>の確率変数は正規分布に従う</h3>
<p>お嬢さん：しかし、ここまでだったら、すでに学習した、独立同一分布からの確率変数の相加平均の確率変数の性質を述べてるだけですよね。平均<span class="math inline"><em>μ</em></span>、分散<span class="math inline"><em>σ</em><sup>2</sup></span>の母集団からの確率変数<span class="math inline"><em>X</em><sub>1</sub>⋯<em>X</em><sub><em>n</em></sub></span>の相加平均の確率変数<span class="math inline"><em>X̄</em></span>の平均<span class="math inline"><em>E</em>(<em>X̄</em>)</span>が<span class="math inline"><em>μ</em></span>、分散<span class="math inline"><em>V</em>(<em>X̄</em>)</span>が<span class="math inline">$\frac{\sigma^2}{n}$</span>になるっていう。</p>
<p>おばあさん：するどい指摘です。つまり、中心極限定理を中心極限定理たらしめている性質はまだ説明されていないのではないかと。</p>
<p>お嬢さん：そのとおりです。</p>
<p>おばあさん：じゃあお見せしましょう。中心極限定理は、<span class="math inline"><em>X̄</em></span>の確率変数が平均<span class="math inline"><em>μ</em></span>、分散<span class="math inline">$\frac{\sigma^2}{n}$</span>の分布をする、というだけではなく、じつは<span class="math inline"><em>X̄</em></span>の確率変数は正規分布<span class="math inline">$N(\mu, \frac{\sigma^2}{n})$</span>にしたがう、ということをいっている定理なのです。</p>
<p>お嬢さん：ほう、正規分布ですか…。</p>
<p>おばあさん：なんで正規分布になるのか、その証明は、今回の私の任務を超えていますので割愛させていただきます。</p>
<p>お嬢さん：そもそもわかってるんですか？</p>
<p>おばあさん：いや、ほぼわかってません。次回までに勉強してきます。で、<span class="math inline"><em>X̄</em></span>の確率変数が正規分布することには大きなメリットがあるんです。</p>
<p>お嬢さん：ほう。</p>
<h3 id="なぜbarxの確率変数が正規分布に従うとうれしいのか">なぜ<span class="math inline"><em>X̄</em></span>の確率変数が正規分布に従うとうれしいのか</h3>
<p>おばあさん：<span class="math inline"><em>X̄</em></span>の確率変数が正規分布するメリットを順を追って説明したいと思います。お嬢さん、いままでの説明で、本当に中心極限定理が使えるものなのかどうか、まだ腑に落ちてないんじゃないですか？</p>
<p>お嬢さん：こういう趣旨のことをいうのは3度目ですが、いくらサンプルサイズ<span class="math inline"><em>n</em></span>が大きくても、あくまで現実ではサンプリングは1回きりなのだから、<span class="math inline"><em>X̄</em></span>が<span class="math inline"><em>μ</em></span>と必ず一致するとは言い切れないという点がやはり引っかかりますね。</p>
<p>おばあさん：それは全くそのとおりで、実際には<span class="math inline"><em>μ</em></span>は<span class="math inline"><em>X̄</em></span>より少し小さかったり、大きかったりすることがほとんどでしょう。測定をどこで切り上げるかにもよりますが。したがって、実際には「<span class="math inline"><em>μ</em></span>は<span class="math inline"><em>X̄</em></span>の周辺に位置する可能性が高い」というような言い方しかできません。</p>
<p>お嬢さん：「周辺」に位置する可能性が「高い」って随分曖昧な言い方ですね。「周辺」ってどのあたりなんだ。そして「高い」ってのも定量化してほしいですね。こんなんじゃやっぱり、中心極限定理もあんまり使えないんじゃないか？</p>
<p>おばあさん：安心してください。<span class="math inline"><em>X̄</em></span>の確率変数は正規分布<span class="math inline">$N(\mu, \frac{\sigma^2}{n})$</span>に従うわけですが、<span class="math inline"><em>X̄</em></span>を線形変換することで標準正規分布<span class="math inline"><em>N</em>(0, 1)</span>に従う確率変数（標準化変数）に変換できます。標準正規分布の累積分布関数は広く知られているので、これを用いて<span class="math inline"><em>X̄</em></span>の「周辺」のどの程度の範囲がどの程度の確率、つまり<span class="math inline"><em>μ</em></span>が含まれうる「可能性の高さ」を持っているのかを定量化できます。</p>
<p>お嬢さん：いいですね。標準化っていう作業ですね。一般に確率変数<span class="math inline"><em>X</em></span>の標準化変数<span class="math inline"><em>Z</em></span>は</p>
<p><span class="math inline">$Z = \frac{X - E(X)}{\sqrt{V(X)}}$</span></p>
<p>で求まり、今回は確率変数<span class="math inline"><em>X̄</em></span>について<span class="math inline">$E(\bar{X})=\mu, V(\bar{X})=\frac{\sigma^2}{n}$</span>なので、</p>
<p><span class="math inline">$Z = \frac{\bar{X} - \mu}{\sigma / \sqrt{n}}$</span></p>
<p>で変換できますね。</p>
<p>おばあさん：さよう。このつづきは再度。</p>
<p>お嬢さん：では、内容をまとめておきます。</p>
<h2 id="まとめ中心極限定理とは">まとめ：中心極限定理とは</h2>
<ol type="1">
<li><p><strong>中心極限定理</strong>とは、平均<span class="math inline"><em>μ</em></span>、分散<span class="math inline"><em>σ</em><sup>2</sup></span>の確率変数をもつ母集団からサンプルサイズ<span class="math inline"><em>n</em></span>の標本をとり、標本平均<span class="math inline">$\bar{X} = \frac{X_1 + \cdots + X_n}{n}$</span>を記録する作業を（たとえば100回）繰り返し、（その100個の）<span class="math inline"><em>X̄</em></span>をひとつの確率変数とみなすと、<span class="math inline"><em>X̄</em></span>の確率変数の確率分布が<span class="math inline">$N(\mu, \frac{\sigma^2}{n})$</span>に従うという定理である。</p></li>
<li><p>実際には標本をとる作業は1回しかできないが、標本を1回だけとっても、<span class="math inline"><em>n</em></span>が十分に大きければ<span class="math inline"><em>X̄</em></span>は母平均<span class="math inline"><em>μ</em></span>の付近の値である可能性が高い。</p></li>
<li><p><span class="math inline"><em>n</em></span>が小さめでも平均的には<span class="math inline"><em>X̄</em></span>は<span class="math inline"><em>μ</em></span>に一致するが、分散<span class="math inline">$V(\bar{X}) = \frac{\sigma^2}{n}$</span>が大きめになるため<span class="math inline"><em>μ</em></span>の推定は不正確になる。だからある程度<span class="math inline"><em>n</em></span>が大きいほうが、<span class="math inline"><em>μ</em></span>の推定の精度は上がる。</p></li>
</ol>
<h2 id="補足混同しやすい点">補足：混同しやすい点</h2>
<ol type="1">
<li><p>中心極限定理に関係するのは標本平均<span class="math inline"><em>X̄</em> = <em>μ</em></span>のみであり、標本分散</p>
<p><span class="math inline">$\frac{1}{n-1}\{(X_1 - \bar{X})^2 + (X_2 - \bar{X})^2 + \cdots + (X_n - \bar{X})^2\} = s^2$</span></p>
<p>は関係しない。中心極限定理でいう「標本平均の分布の分散」は<span class="math inline"><em>V</em>(<em>X̄</em>)</span>のことを指しており「標本分散」<span class="math inline"><em>s</em><sup>2</sup></span>のことではない。</p>
<p>それにしても混同しやすいが、この混同しやすさは、「標本平均の分布」と「標本分布」の混同しやすさから来ていると思う。前者は実質的に仮想的な概念だが、後者は観測可能である、という違いを抑えていれば理解しやすくなるのではないか。</p></li>
<li><p>中心極限定理で重要なのはサンプルサイズ<span class="math inline"><em>n</em></span>の大きさであって、<span class="math inline"><em>X̄</em></span>の確率変数を考えるときに仮想的に導入した標本数（上のまとめでは100とした）ではない。</p>
<p>標本数が増えれば、<span class="math inline"><em>X̄</em></span>が<span class="math inline">$N(\mu, \frac{\sigma^2}{n})$</span>に従う様子が<span class="math inline"><em>X̄</em></span>のヒストグラム上でよりわかりやすくなるだろうが、それだけである。標本数が増えれば<span class="math inline"><em>V</em>(<em>X̄</em>)</span>が小さくなる、というような関係はない。</p></li>
</ol>
</body>
</html>
